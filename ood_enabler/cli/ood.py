/*
 * © Copyright IBM Corp. 2024, and/or its affiliates. All Rights Reserved
 *
 * Licensed under the Apache License, Version 2.0 (the “License”);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *       http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an “AS IS” BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
'''
Created on Mar 29, 2023

@author: 285782897
'''
from argparse import ArgumentParser, RawDescriptionHelpFormatter
import json
import os
import sys
from typing import List

ood_path = os.path.abspath('.')
if ood_path not in sys.path:
    sys.path.append(ood_path)


from ood_enabler.data.data_handler import DataHandler
from ood_enabler.data.factory import get_image_data_handler
from ood_enabler.inference_service.factory import get_inference_service
from ood_enabler.inference_service.inference_service import InferenceService
from ood_enabler.model_wrapper.model import ModelWrapper
from ood_enabler.ood_enabler import OODEnabler
from ood_enabler.storage.local_storage import FileSystemStorage
from ood_enabler.storage.model_store import ModelStore
from ood_enabler.storage.storage import Storage
from ood_enabler.util.constants import InferenceServiceType
from ood_enabler.util.constants import MLBackendType 


def get_dict_arg_value(argval:str):
    t = argval.replace("'",'"')
    d = json.loads(t) 
    return d

def get_model_wrapper(model_store:ModelStore, args:dict) -> ModelWrapper:
    model_metadata = get_dict_arg_value(args.model_metadata)
    model_metadata['type'] =  args.framework
    model_file =  args.model_src_path 
    return model_store.load(model_metadata, model_file)

def get_model_store(args:dict) -> ModelStore:
    apikey = args.cos_apikey
    bucket = args.cos_bucket
    service_instance_id = args.cos_service_instance_id
    service_endpoint = args.cos_service_endpoint
    auth_endpoint = args.cos_auth_endpoint
    if service_endpoint is not None:
        #def __init__(self, bucket, api_key=None, service_instance_id=None, endpoint=None, auth_endpoint=None):
        model_store = ModelStore.from_cos(bucket, apikey, service_instance_id, service_endpoint, auth_endpoint)
    else:
        model_store = ModelStore.from_filesystem()
    return model_store
         

def get_storage() -> Storage:
    return FileSystemStorage()


def get_data_handler(args:dict) -> DataHandler:
    #get_image_data_handler(model_backend, ds_metadata, local_store, source, destination='.'):
    source =  args.data_uri 
    if source is None:
        return None     # User is not requesting normalization on the ood scores

    ds_metadata = get_dict_arg_value(args.data_metadata) 
    # t = args.ds_metadata.replace("'",'"')
    # ds_metadata = json.loads(t) 
    local_store = get_storage()
    destination = '.'
    h = get_image_data_handler(args.framework, ds_metadata, local_store, source, destination)
    return h 

def get_infer_service(args:dict) -> InferenceService:
    t = args.inference_service
    service = get_inference_service(t)
    return service

def get_env(key:str, required:bool, default:object=None, oneof:List=None ) -> object:
    value = os.environ.get(key)
    if value is None:
        if required:
            raise ValueError("Environment variable {} not found, but is required.".format(key))
        else:
            value = default
    if value is not None and oneof is not None:
        if not value in oneof: 
            raise ValueError("Value {} for environment variable {} must be one of {}.".format(value, key, oneof))
    return value

def environ_or_required(key, oneof:List=None):
    envval = get_env(key,False, None,oneof) 
    if envval is None:
        return {'required': True}
    return {'default': envval} 

if __name__ == '__main__':
    parser = ArgumentParser(
        formatter_class=RawDescriptionHelpFormatter,
          #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
        description=
          "Adds out-of-distribution score generation to an existing pytorch or tensorflow\n"
        + "model. A single model is read from Cloud Object Storage (COS) or the file\n"
        + "system, and modified to include an 'ood' capability. \n"
        + "\n"
        + "Optionally, the OOD scores generated by the new model can be normalized into a\n"
        + "fixed range of 0 to 1 by providing a data set that is used to identify a range\n"
        + "of unnormalized OOD scores.   The unnormalized scores are used to develop a \n"
        + "scaling function (based on min/max scores). Producing a normalized model\n"
        + "generally takes more time due to the fact that inferencing on the given\n"
        + "data set is required. When normalizing, and thus inferencing is required, an \n"
        + "inferencing service must be specified. The inferencing service may be a local\n"
        + "in memory service or a kserve-based service.\n"
        + "\n"
        + "Once model modification is completed, the model is stored back to the source of\n"
        + "the loaded model (i.e. COS or file system) under a specified path."
        , epilog="Examples:\n"
        + "\nWith normalization using the file system...\n"
        + "python ood.py --framework pytorch \\\n"
        + "     --model_src_path resnet50.pt \\\n"
        + "     --data_uri flower_photos_small.tar.gz \\\n"
        + "     --data_metadata \"{'img_height': 224, 'img_width': 224, 'batch_size': 32, \\\n"
        + "          'normalize': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]}\" \\\n" 
        + "     --model_metadata \"{'type': 'pytorch', 'arch': 'resnet50'}\" \\\n"
        + "     --inference_service in-memory \\\n"
        + "     --model_dest_path ood_resnet50.pt"
        + "\nWithout normalization using the file system...\n"
        + "python ood.py --framework pytorch \\\n"
        + "     --model_src_path resnet50.pt \\\n"
        + "     --model_metadata \"{'type': 'pytorch', 'arch': 'resnet50'}\" \\\n"
        + "     --model_dest_path ood_resnet50.pt"
        + "\nWithout normalization using COS...\n"
        + "python ood.py --framework pytorch \\\n"
        + "     --model_src_path resnet50.pt \\\n"
        + "     --model_metadata \"{'type': 'pytorch', 'arch': 'resnet50'}\" \\\n"
        + "     --model_dest_path ood_resnet50.pt \\\n"
        + "     --cos_service_endpoint https://s3.us-east.cloud-object-storage.appdomain.cloud \\\n"
        + "     --cos_bucket mybucket \\\n" 
        + "     --cos_service_instance_id crn:v1:bluemix:public:cloud-object-storage:global:a/74c6d94213d643b5b53bfbeb3c1e8de0:f9de716e-5b49-41a0-8c10-e0cd4127b192:: \\\n"
        + "     --cos_auth_endpoint https://iam.cloud.ibm.com/oidc/token \\\n"
        + "     --cos_apikey <YOUR APIKEY HERE>"
        )
    requiredNamed = parser.add_argument_group('Required named arguments')
    optionalNamed = parser.add_argument_group('Optional named arguments')
    requiredNamed.add_argument('--framework', type=str, 
                        help="The modeling framework to use. One of {} or {}.  Can also be set using the OOD_FRAMEWORK env var.".format(MLBackendType.PYTORCH, MLBackendType.TF),
                        **environ_or_required('OOD_FRAMEWORK', [MLBackendType.PYTORCH, MLBackendType.TF]))
    requiredNamed.add_argument('--model_src_path', type=str, 
                        **environ_or_required('OOD_MODEL_SRC_PATH'),
                        help="The file system location of the model file.  Can also be set using the OOD_MODEL_SRC_PATH env var.")
    requiredNamed.add_argument('--model_metadata', type=str, 
                        **environ_or_required('OOD_MODEL_METADATA'),
                        help="Framework-specific metadata dictionary describing the model, especially the 'arch' type.  Can also be set using the OOD_MODEL_METADATA env var.")
    optionalNamed.add_argument('--data_uri', type=str, 
                        required=False, default=get_env('OOD_DATA_URI',False,None,None),
                        help="Specifies the location of the data set used in normalizing OOD scores.  If not specified, normalization will not be performed."
                        + "  Can also be specified with the OOD_DATA_URI env var.")
    optionalNamed.add_argument('--data_metadata', type=str, 
                        required=False, default=get_env('OOD_DATA_METADATA',False,None,None),
                        help="Specifies the information about the data set using a dictionary (i.e. image size, normalization, etc)."
                         + " Generally required if a data URI is given. Can also be set by the OOD_DATA_METADATA env var.") 
    optionalNamed.add_argument('--inference_service', type=str, 
                        required=False, 
                        default=get_env('OOD_INFERENCE_SERVICE',False,InferenceServiceType.IN_MEMORY,[InferenceServiceType.KSERVE, InferenceServiceType.IN_MEMORY ]),
                        help="The type of inference service to use when normalizing the data. One of {} or {}.  Can also be set by the OOD_INFERENCE_SERVICE env var."
                        + "  When specifying kserve, additional env vars (url, storage, credentials, etc.) must be specified to configure the kserve connection."
                        .format(InferenceServiceType.KSERVE, InferenceServiceType.IN_MEMORY))
    requiredNamed.add_argument('--model_dest_path', type=str, 
                        **environ_or_required('OOD_MODEL_DEST_PATH'),
                        help="The file system location (currently a directory) where the new model is to be stored. Can also be set by the OOD_MODEL_DEST_PATH env var.")

    optionalNamed.add_argument('--cos_service_endpoint', type=str, 
                        default=get_env('OOD_COS_SERVICE_ENDPOINT',False,None,None),
                        help="Defines Cloud Object Storage (COS) as the model storage, both for loading and storing of the new model."
                        + " The value given is a URI for the service."
                        + " If not specified, file system-based model storage is used."
                        + " Can also be set by the OOD_COS_SERVICE_ENDPOINT env var.")
    optionalNamed.add_argument('--cos_bucket', type=str, 
                        default=get_env('OOD_COS_BUCKET',False,None,None),
                        help="Defines the COS bucket to retrieve/store models  from/to."
                        + " Only used if the COS service endpoint is set."
                        + " Can also be set by the OOD_COS_BUCKET env var.")
    optionalNamed.add_argument('--cos_auth_endpoint', type=str, 
                        default=get_env('OOD_COS_AUTH_ENDPOINT',False,None,None),
                        help="Defines the COS authentication endpoint URI."
                        + " Only used if the COS service endpoint is set."
                        + " Can also be set by the OOD_COS_AUTH_ENDPOINT env var.")
    optionalNamed.add_argument('--cos_apikey', type=str, 
                        default=get_env('OOD_COS_APIKEY',False,None,None),
                        help="Defines the COS authentication api key."
                        + " Only used if the COS service endpoint is set."
                        + " Can also be set by the OOD_COS_APIKEY env var.")
    optionalNamed.add_argument('--cos_service_instance_id', type=str, 
                        default=get_env('OOD_COS_SERVICE_INSTANCE_ID',False,None,None),
                        help="Defines the COS service instance URI."
                        + " Only used if the COS service endpoint is set."
                        + " Can also be set by the OOD_COS_SERVICE_INSTANCE_ID env var.")


    args = parser.parse_args()
    model_store = get_model_store(args)
    data_handler = get_data_handler(args)
    source_model = get_model_wrapper(model_store, args)
    inference_service = get_infer_service(args)

    ood = OODEnabler()
    ood_model = ood.ood_enable(source_model, data_handler, inference_service)
    if ood_model is None:
        ood_model = source_model
    model_store.upload(ood_model, args.model_dest_path)
    
